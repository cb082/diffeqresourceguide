<h1>Polynomial Systems and Context-free Grammars</h1>

<p>
    Last time, we saw that a couple of very simple languages... balancing parentheses, same amounts of \(a\)s as \(b\), etc... are nonregular! 
    By Kleene's Theorem, this amounts to the statement that there is no state in any finite automaton that accepts them (how rude!).
    The general slogan you hear people say about these languages is that they "require memory to recognize".
    This is true (and in a precise sense), but I think it is much easier to understand these languages from a systems-of-equations perspective.
    So that's what we are going to do today.
</p>
<p>
    We have already seen one way of generalizing our left-affine systems of equations: by allowing them to be unguarded (for there to be terms \(r_{ij}x_i\) where \(r_{ij}\) <i>does</i> have the empty word property). 
    Because of the dagger construction, we can always transform one of these unguarded systems into a guarded one, so this generalization didn't buy us a larger family of languages than \(\mathsf{Rel}\).
    Today we are going to generalize even more: we're going to let go of "left-affine" and replace it with "polynomial". 
    This will lead us to a much bigger family of languages, including some languages you will be <i>very</i> familiar with.
</p>

<div class="example">
    <b>(A = B: Resurrection)</b>
    Let's start with a simple example and return to the language 
    \[
        L_{a = b} = \{a^n b^n \mid n \in \mathbb N\}
    \]
    from before.
    We already saw in the previous lecture that \(L \notin \mathsf{Reg}\) (we used the Pumping Lemma for this). 
    But it can still be written as a least solution to a system of equations. 
    In fact, it is the least solution to a single equation: 
    \[
        x = \varepsilon + a x b
    \]
    Let's see how this works.

    <p>
        Remember that a <i>solution</i> to this equation is a language \(L \subseteq A^*\) such that 
        \[
            L = \{\varepsilon\} \cup (\{a\} \cdot L \cdot \{b\})
            \hspace{4em}\text{(*)}
        \]
        and that the <i>least</i> solution is the solution that is contained in all other solutions. 
        The language \(L_{a = b}\) satisfies this equation: we can read directly from \(\text{(*)}\) that \(\varepsilon \in L_{a = b}\) (set \(n = 0\)) and for any \(n \in \mathbb N\), \(a(a^nb^n)b = a^{n+1}b^{n+1} \in L_{a=b}\).
    </p>
    <p>
        To see that \(L_{a = b}\) is the <i>least</i> solution, we need to check that it <i>only contains the words it has to</i>. 
        To that end, let's consider an arbitrary solution \(L\).
        We know that \(\varepsilon \in L\), because the right-hand side explicitly requires \(\varepsilon \in L\). 
        Now, since \(\varepsilon \in L\), the second term on the right-hand side tells us that 
        \[
            a \varepsilon b = ab \in L
        \]
        Again, since \(ab \in L\), the equation also tells us that 
        \[
            a(ab)b = a^2b^2 \in L
        \]
        Continuing to plug words in \(L\) back into \(a \cdot (-) \cdot b\), it's not too hard to see that \(L_{a = b} \subseteq L\).
        This tells us that \(L_{a = b}\) is the smallest language that satisfies this equation!
    </p>
</div>

<p>
    In the example above, we just argued something very important: going from "left-affine" to "polynomial" <i>does</i> buy us more languages!
    That is, there are polynomial systems of equations whose least solutions are <i>not</i> regular. 
</p>

<div class="individual-exercise">
    <b>(A = B: Resurrection: Resurrection)</b>
    Rephrase the reasoning in the example above so that it is an explicit proof by induction of the inclusion \(L_{a = b} \subseteq L\) for any solution \(L\) to \(x = \varepsilon + axb\).
    <div class="hint">
        This is a proof by induction on \(n\): the goal is to show that \(a^nb^n \in L\) for all \(n \in \mathbb N\).
        The base case has been done for you in the example.
    </div>
</div>

<div class="definition">
    <b>(Polynomial Expressions)</b>
    Let \(A\) be an alphabet of input letters, and let \(X = \{x_1, \cdots, x_n\}\) be a set of <i>variables</i> (which you can think of as taking languages as values).
    A <i>polynomial expression (over \(A\)) in \(X\) variables</i> is an expression formed from the following formation rules:
    <ol>
        <li>\(\emptyset\), \(\varepsilon\), and \(a\) are polynomials for any \(a \in A\)</li>
        <li>\(x\) is a polynomial expression for every \(x \in X\)</li>
        <li>If \(p_1(x_1, \dots, x_n), p_2(x_1, \dots, x_n)\) are polynomial expressions, then so are 
            <ol type="i">
                <li>\(p_1(x_1, \dots, x_n) + p_2(x_1, \dots, x_n)\)</li>
                <li>\(p_1(x_1, \dots, x_n) \cdot p_2(x_1, \dots, x_n)\)</li>
            </ol>
        </li>
    </ol>
    Above, we write \(p(x_1, \dots, x_n)\) to emphasize that the variables in the polynomial expression that appear are \(x_1, \dots, x_n \in X\). 
    The set of all polynomial expressions in \(X\) variables is written \(\mathit{Poly}(X)\).

    <p>
        A <i>polynomial system of equations in \(X\) variables</i> is a system of equations of the form 
        \[\begin{aligned}
            x_1 &= p_1(x_1, \dots, x_n) \\
            &\ \vdots \\
            x_n &= p_n(x_1, \dots, x_n)
        \end{aligned}\]
        where \(x_1, \dots, x_n \in X\) and \(p_i(x_1,\dots, x_n) \in \mathit{Poly}(X)\) for each \(i=1,\dots, n\). 
        A <i>solution</i> to the polynomial system of equations in \(X\) variables above is a sequence of languages \(L_1, \dots, L_n\) such that 
        \[\begin{aligned}
            L_1 &= p_1(L_1, \dots, L_n) \\
            &\ \vdots \\
            L_n &= p_n(L_1, \dots, L_n)
        \end{aligned}\]
        That is, when you plug \(L_i\) into \(x_i\) for each \(x_i \in X\), then the equations between languages you obtain are true. 
        A <i>least solution</i> to the polynomial system of equations in \(X\) variables above is a solution \(U_1, \dots, U_n\) such that for any solution \(L_1, \dots, L_n\), \(U_i \subseteq L_i\) for all \(i = 1, \dots, n\).
    </p>
</div>

<p>
    The following individual exercise shows us that every regular language is the least solution to some polynomial system of equations.
</p>

<div class="individual-exercise">
    <b>(Left-Affine is Polynomial)</b>
    Show that every left-affine system of equations is a polynomial system of equations.
    Use Kleene's Theorem to explain why this means that every regular language is the least solution to some polynomial system of equations.
    <div class="hint">
        Set up the system of equations by using the formation rules for polynomial expressions to obtain left-affine equations.
    </div>
</div>

<div class="exercise">
    <b>(A=?)</b>
    Use the formation rules for polynomial expressions to write the polynomial 
    \[
        p(x) = (\varepsilon + a(x + b)x)b
        <!-- b + a(x + b)xb = b + axxb + abxb -->
    \]
    Now find \(5\) different words that appear in the least solution to the polynomial equation \(x = p(x)\) in \(\{x\}\) variables. 
    <div class="hint">
        Use Salomaa's axioms to simplify the polynomial up to language equivalence.
        What's the smallest word in the language?
    </div>
</div>

<div class="exercise">
    <b>(A=?)</b>
    Use the formation rules for polynomial expressions to write the polynomial 
    \[
        p(x) = \varepsilon + (a + b)x(a + b)
        <!-- \varepsilon + axa + axb + bxa + bxb -->
    \]
    Describe the least solution to the equation.
    <div class="hint">
        Use Salomaa's axioms to simplify the polynomial up to language equivalence.
        What's the smallest word in the language?
    </div>
</div>

<p>
    The thing about polynomial systems is that they're hard to solve.
    And unfortunately, Arden's Rules can't save us when it comes to finding these least solutions.
    We need to use a slightly different but more direct approach.
    This approach requires us to show that we can write polynomial expressions a certain way.
</p>

<div class="definition">
    <b>(Monomial)</b>
    Let \(A\) be an alphabet and \(X\) be a set of variables and assume that \(A \cap X = \{\}\) (there is no overlap between input letters and variables---this is an assumption we will always make).
    Then a <i>monomial expression</i> is a word \(\mu \in (X \cup A)^*\). 
    We write \(\mu(x_1, \dots, x_n)\) to emphasize that \(x_1, \dots, x_n\) are the variables that appear in \(\mu\).
</div>

<p>
    The set \((X \cup A)^*\) consists of all of the words you can form by expanding your alphabet to include \(X\).
    In other words, monomials are polynomials formed without using the \(+\) formation rule.
</p>

<div class="definition">
    <b>(Grammar Ready)</b>
    Given a polynomial expression \(p(x_1, \dots, x_n)\) over \(A\) in \(X\) variables, we say that <i>\(p\) is in grammar-ready form</i> if there are monomial expressions \(\mu_1, \dots, \mu_k \in (X \cup A)^*\) such that 
    \[
        p = \mu_1 + \mu_2 + \dots + \mu_k
    \]
    (Note that this is syntactic equality, not languguage equivalence.)
    A polynomial system of equations is <i>grammar-ready</i> if every polynomial expression that appears in it is in grammar-ready form.
</div>

<p>
    For example, all of
    \[
        axb
        \qquad 
        xxyabxybc + \varepsilon
        \qquad
        by + \varepsilon x y a
    \]
    are in grammar-ready form, but  
    \[
        ax(b + y)
        \qquad 
        xx(a + b)xybc + \varepsilon
        \qquad
        b(y + \varepsilon) x y a
    \]
    are not.
    However, every polynomial expression has a grammar-ready equivalent.
</p>

<div class="exercise">
    <b>(Getting Ready for School)</b>
    For each polynomial expression below, 
    \[
        p_1(x, y) = ax(b + y)
        \qquad 
        p_2(x, y) = xx(a + b)xybc + \varepsilon
        \qquad
        p_3(x, y) = b(y + \varepsilon) x y a
    \]
    find a language equivalent polynomial expression in grammar-ready form, i.e., \(q_i(x, y)\) that is in grammar-ready form and 
    \[
        p_i(x, y) =_{\mathcal L} q_i(x, y)
    \]    
</div>

<p>
    As you can probably guess, the translation from non-grammar-ready to grammar-ready is always the same sequence of steps.
    This is the basic idea behind proving the following lemma. 
</p>

<div class="lemma">
    <b>(Grammar-Readiness)</b>
    Every polynomial system of equations has the same least solution as a grammar-ready polynomial system of equations.
</div>

<!--Section-->
<h2>Grammars</h2>

<p>
    Finding the least solution to a polynomial system of equations is always a matter of determining which words <i>have to be in every solution</i>.
    This is always a matter of plugging in words we have already determined are in the language into the right-hand side of the equation. 
    There is a more systematic way of writing this process down: it's called a <i>rewrite rule</i>.
    Roughly speaking, a set of rewrite rules is called a <i>grammar</i>.
</p>

<div class="definition">
    <b>(Context-Free Grammar)</b>
    A <i>context-free grammar</i> (or simply put, a <i>grammar</i>) is a tuple \(\mathcal G = (X, A, R)\) consisting of 
    <ul>
        <li>A set \(X\) of <i>variables</i></li>
        <li>An alphabet \(A\) of letters</li>
        <li>A set of <i>rewrite rules</i> \(R\), which relates variables to monomial expressions over \(A\) in \(X\) variables,
            \[
                R \subseteq X \times (X \cup A)^*
            \]
        </li>
    </ul>
    Given a variable \(x \in X\) and a monomial expression \(\mu\), we usually write \(x \to_R \mu\) to mean \((x, \mu) \in R\).
    If \(X\) and \(R\) are finite, then we say that \(\mathcal G\) is <i>finite</i>.
</div>

<p>
    Rewrite rules are essentially just translations of variables into words. 
    This translation extends to a larger rewriting system that allows us to transform words into words.
</p>

<div class="definition">
    <b>(Derivation)</b>
    Let \(\mathcal G = (X, A, \Rightarrow)\) be a grammar.
    The <i>rewrite relation in \(\mathcal G\)</i> is the relation 
    \[
        {\Rightarrow_{\mathcal G}} \subseteq (X \cup A)^* \times (X \cup A)^*
    \] 
    defined as follows: for any monomial expressions \(\mu_1(x_1, \cdots, x_n), \mu_2(x_1, \dots, x_n) \in (X \cup A)^*\), \(\mu_1 \Rightarrow_{\mathcal G} \mu_2\) if and only if there is a rewrite rule \(x_i \to_R \tau\) and 
    \[
        \mu_2 = \mu_1(x_1, \dots, \tau, \dots, x_n)
    \]
    The <i>multistep rewrite relation in \(\mathcal G\)</i> is the relation \(\Rightarrow_{\mathcal G}^*\) defined so that for any monomial expressions \(\mu\) and \(\tau\), \(\mu \Rightarrow_{\mathcal G} \tau\) if and only if there is a sequence of rewrites
    \[
        \mu 
        \Rightarrow_{\mathcal G} \mu_1 
        \Rightarrow_{\mathcal G} \cdots 
        \Rightarrow_{\mathcal G} \mu_n = \tau
    \]
    We say that \(\mu \Rightarrow_{\mathcal G}^* \tau\) is a <i>derivation</i> if \(\tau\) contains no variables, i.e., \(\tau = w \in A^*\) is a word from \(A\). 
</div>

<div class="example">
    For example, consider the grammar \[
        \mathcal G = (\{x\}, \{a, b\}, \{(x, \varepsilon), (x, axb)\})
    \]
    This grammar has \(X = \{x\}\) for its single variable, \(A = \{a, b\}\) for its alphabet, and two rewrite rules, \(x \to \varepsilon\) and \(x \to axb\). 
    What these rewrite rules are trying to capture are the possible ways that you can <i>start with the string \(x\) and rewrite it by repeatedly using the rules</i>.
    In \(\mathcal G\), we can derive the string \(aabb\) by applying the rewrite rules as follows:
    </i>
    \[
        x 
        \overbrace{\Rightarrow_{\mathcal G}}^{x \to axb} axb 
        \overbrace{\Rightarrow_{\mathcal G}}^{x \to axb} aaxbb 
        \overbrace{\Rightarrow_{\mathcal G}}^{x \to \varepsilon} aa\varepsilon bb = aabb
    \]
    This sequence of rewrites is a derivation because \(aabb\) is a word in \(A^*\).
</div>

<div class="definition">
    <b>(Derived, Context-free)</b>
    Given a context-free grammar \(\mathcal G = (X, A, R)\) and a variable \(x \in X\), the <i>language derived from \(x\)</i> is the language 
    \[
        \mathcal L(\mathcal G, x)
        = \{w \in A^* \mid \text{there is a derivation } x \Rightarrow_{\mathcal G}^* w\}
    \]
    A language \(L \subseteq A^*\) is called a <i>context-free language</i> if there is a contextfree gramma \(\mathcal G\) with a variable \(x\) such that \(\mathcal L(\mathcal G, x) = L\), i.e., it is the language derived from a variable in a finite context-free grammar.

    <p>
        The family of context-free languages is written 
        \[
            \mathsf{CFL} = \{L \subseteq A^* \mid L\text{ is context-free} \}
        \]
    </p>
</div>

<div class="exercise">
    <b></b>

</div>

<h2>Backus–Naur Form</h2>

<p>
    Grammars achieve the same goal as "formation rules", as we have been using the phrase. 
</p>